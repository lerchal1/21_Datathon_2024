{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from ..utils.models.lstm import LSTM\n",
    "from ..utils.models.cnn import ConvRNN\n",
    "from ..utils.metrics import growth_metric\n",
    "from torch.optim import SGD, Adam\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "Setting the seeds to allow reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds and device\n",
    "seed = 2\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Load two different datasets, one with values previous 2022 and one with values after. \\\n",
    "The idea is to first train the model on the data previous 2022 and then fine-tune it on data after 2022\n",
    "\n",
    "The way the data is structured is as follow:\n",
    "* The data is normalized per column\n",
    "* We have the data sorted by \"business_entity_doing_business_as_name\", \"period_end_date\"\n",
    "* We explore the dataframe row by row with window_length=10 and K=1\n",
    "* The row in the window_length are the input for our model, while the future up to K steps is what the model has to predict\n",
    "* The idea is that up to K steps we define a metric, which you can find under utils/metrics.py. Nutshell: this metric describes the ratio between interactions/#posts. Then the metric is applied to these K weeks immediately after the window, to produce a label (scalar value), which the model can be trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import get_datasets\n",
    "\n",
    "path_Full = \"../data/Full_Feature_data.csv\"\n",
    "path_Before2022 = \"../data/Before2022_Feature_data.csv\"\n",
    "path_From2022 = \"../data/From2022_Feature_data.csv\"\n",
    "\n",
    "train_dataset_From2022, val_dataset_From2022 = get_datasets(path_From2022, test_size=0.1)\n",
    "train_dataset_Before2022, val_dataset_Before2022 = get_datasets(path_Before2022, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader_Before2022 = DataLoader(train_dataset_Before2022, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "val_loader_Before2022 = DataLoader(val_dataset_Before2022, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "train_loader_From2022 = DataLoader(train_dataset_From2022, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "val_loader_From2022 = DataLoader(val_dataset_From2022, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models\n",
    "The two models that we try are a simple LSTM implementation and a ConvRNN. \\\n",
    "The idea is that the models have to capture dependencies inside the window of week to predict the future metric value. \\\n",
    "Hence, we opt for two models which have the right bias to capture this. \\\n",
    "We use as a Loss a simple Mean Squared Error and we evaluate the prediction still with the Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and optimizer\n",
    "model = ConvRNN(train_dataset_From2022[0][0].shape[1], train_dataset_From2022[0][0].shape[0], 1, n_channels1=128, n_channels2=128, n_channels3=128, n_units1=128, n_units2=128, n_units3=128)\n",
    "#model = LSTM()\n",
    "optimizer = Adam(model.parameters(), lr=0.01)\n",
    "loss = torch.nn.functional.mse_loss\n",
    "\n",
    "cfg = {\n",
    "        \"model\": model,\n",
    "        \"setup\": \"train\",\n",
    "        \"loss\": loss,\n",
    "        \"optimizer\": optimizer,\n",
    "        \"epochs\": 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.experiments import Experiment\n",
    "\n",
    "model = Experiment(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='./',\n",
    "    filename='{epoch:02d}-{val_loss:.2f}'\n",
    ")\n",
    "\n",
    "class PrintCallback(pl.Callback):\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        print(f\"Training loss: {trainer.callback_metrics['train_loss']}\")\n",
    "        \n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        print(f\"Validation loss: {trainer.callback_metrics['val_loss']}, Mse: {trainer.callback_metrics['val_mse']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(accelerator=\"cpu\", max_epochs=cfg[\"epochs\"], callbacks=[PrintCallback()])#, EarlyStopping(monitor=\"val_loss\", mode=\"min\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train before 2022\n",
    "Here we train the model for some epochs on the dataset before2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, train_loader_Before2022, val_loader_Before2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train after 2022\n",
    "The same model is then trained on the dataset after2022 to make it more relevant for the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, train_loader_From2022, val_loader_From2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose a particular brand of interest where we want to infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path_From2022, sep=\",\")\n",
    "df = df[df[\"business_entity_doing_business_as_name\"] == \"Calvin Klein\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import BrandDataset\n",
    "infer_data_set = BrandDataset(df)\n",
    "infer_loader = DataLoader(infer_data_set, batch_size=1, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important !\n",
    "The prediction can be used inside a Test to check whether the brand is an outlier compared to its previous trend. \\\n",
    "To do that we proceed as follow:\n",
    "* We evaluate the metric over the window_length to generate an *avg_growth* in the past period.  Moreover we compute the *std_dev_growth* from the metric growth for each week inside our window_length. Our model is then making the prediction *growth* of the metric for the future in the next K=1 weeks\n",
    "* We then check whether *growth* - *avg_growth* > z*std_dev_growth*, z tunable (ex. z=2 means in 95.47% positive outlier) to detect whether we have a **POSITIVE OUTLIER**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "for x, y in infer_loader:\n",
    "        out = model.forward(x)    \n",
    "        loss = torch.nn.functional.mse_loss(out.squeeze(), y.squeeze())   \n",
    "\n",
    "        print(\"train_loss {}\".format(loss))\n",
    "        mse = mean_squared_error(y.cpu().numpy(), out.cpu().detach().numpy())\n",
    "        print(\"train_mse {}\".format(mse))\n",
    "        print(out)\n",
    "        print(y)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
