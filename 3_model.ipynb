{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.models.lstm import LSTM\n",
    "from utils.metrics import growth_metric\n",
    "from torch.optim import SGD, Adam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n"
     ]
    }
   ],
   "source": [
    "# Set seeds and device\n",
    "seed = 2\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/skylab_instagram_datathon_dataset_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "from utils.dataset import get_datasets\n",
    "\n",
    "path_Full = \"data/Full_Feature_data.csv\"\n",
    "path_Before2022 = \"data/Before2022_Feature_data.csv\"\n",
    "path_From2022 = \"data/From2022_Feature_data.csv\"\n",
    "\n",
    "train_dataset, val_dataset = get_datasets(path, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and optimizer\n",
    "model = LSTM()\n",
    "optimizer = Adam(model.parameters(), lr=0.01)\n",
    "loss = torch.nn.functional.mse_loss\n",
    "\n",
    "cfg = {\n",
    "        \"model\": LSTM(),\n",
    "        \"setup\": \"train\",\n",
    "        \"loss\": loss,\n",
    "        \"optimizer\": optimizer,\n",
    "        \"epochs\": 10,\n",
    "        \"data_loader\": train_loader,\n",
    "        \"val_data\": val_loader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.experiments import Experiment\n",
    "\n",
    "model = Experiment(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='./',\n",
    "    filename='{epoch:02d}-{val_loss:.2f}'\n",
    ")\n",
    "\n",
    "class PrintCallback(pl.Callback):\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        print(f\"Training loss: {trainer.callback_metrics['train_loss']}\")\n",
    "        \n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        print(f\"Validation loss: {trainer.callback_metrics['val_loss']}, Mse: {trainer.callback_metrics['val_mse']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ggil/anaconda3/envs/hackathon/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(accelerator=\"cpu\", max_epochs=cfg[\"epochs\"], callbacks=[PrintCallback()])#, EarlyStopping(monitor=\"val_loss\", mode=\"min\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ggil/anaconda3/envs/hackathon/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py:181: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | LSTM | 798 K \n",
      "-------------------------------\n",
      "798 K     Trainable params\n",
      "0         Non-trainable params\n",
      "798 K     Total params\n",
      "3.193     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 52.97it/s]Validation loss: 1.5871280695738368e+16, Mse: 1.5871282843222016e+16\n",
      "Epoch 0: 100%|██████████| 202/202 [00:06<00:00, 30.38it/s, v_num=8]        Validation loss: 5.480022577684414e+18, Mse: 5.480022577684414e+18\n",
      "Epoch 0: 100%|██████████| 202/202 [00:07<00:00, 28.71it/s, v_num=8, val_loss=5.48e+18]Training loss: 6572716765216768.0\n",
      "Epoch 1: 100%|██████████| 202/202 [00:06<00:00, 28.94it/s, v_num=8, val_loss=5.48e+18]Validation loss: 5.480022577684414e+18, Mse: 5.480022577684414e+18\n",
      "Epoch 1: 100%|██████████| 202/202 [00:07<00:00, 26.58it/s, v_num=8, val_loss=5.48e+18]Training loss: 1.9929694077976576e+16\n",
      "Epoch 2: 100%|██████████| 202/202 [00:06<00:00, 29.56it/s, v_num=8, val_loss=5.48e+18]Validation loss: 5.480022577684414e+18, Mse: 5.480022577684414e+18\n",
      "Epoch 2: 100%|██████████| 202/202 [00:08<00:00, 23.98it/s, v_num=8, val_loss=5.48e+18]Training loss: 3225094163791872.0\n",
      "Epoch 3:  46%|████▌     | 93/202 [00:03<00:03, 27.80it/s, v_num=8, val_loss=5.48e+18] "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "trainer.fit(model, cfg[\"data_loader\"], cfg[\"val_data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
