{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.models.lstm import LSTM\n",
    "from utils.metrics import growth_metric\n",
    "from torch.optim import SGD, Adam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n"
     ]
    }
   ],
   "source": [
    "# Set seeds and device\n",
    "seed = 2\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/Feature_data.csv\n",
      "We have 0/325732 iters\n"
     ]
    }
   ],
   "source": [
    "from utils.dataset import get_datasets\n",
    "\n",
    "path_Full = \"data/Feature_data.csv\"\n",
    "path_Before2022 = \"data/Before2022_Feature_data.csv\"\n",
    "path_From2022 = \"data/From2022_Feature_data.csv\"\n",
    "\n",
    "train_dataset, val_dataset = get_datasets(path_Full, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 16])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and optimizer\n",
    "model = LSTM()\n",
    "optimizer = Adam(model.parameters(), lr=0.01)\n",
    "loss = torch.nn.functional.mse_loss\n",
    "\n",
    "cfg = {\n",
    "        \"model\": model,\n",
    "        \"setup\": \"train\",\n",
    "        \"loss\": loss,\n",
    "        \"optimizer\": optimizer,\n",
    "        \"epochs\": 10,\n",
    "        \"data_loader\": train_loader,\n",
    "        \"val_data\": val_loader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.experiments import Experiment\n",
    "\n",
    "model = Experiment(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='./',\n",
    "    filename='{epoch:02d}-{val_loss:.2f}'\n",
    ")\n",
    "\n",
    "class PrintCallback(pl.Callback):\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        print(f\"Training loss: {trainer.callback_metrics['train_loss']}\")\n",
    "        \n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        print(f\"Validation loss: {trainer.callback_metrics['val_loss']}, Mse: {trainer.callback_metrics['val_mse']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First train over 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ggil/anaconda3/envs/hackathon/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "cfg[\"data_loader\"] = train_loader\n",
    "cfg[\"val_loader\"] = val_loader\n",
    "trainer = pl.Trainer(accelerator=\"cpu\", max_epochs=cfg[\"epochs\"], callbacks=[PrintCallback()])#, EarlyStopping(monitor=\"val_loss\", mode=\"min\")])\n",
    "trainer.fit(model, cfg[\"data_loader\"], cfg[\"val_data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Finetune on dataset after 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ggil/anaconda3/envs/hackathon/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py:181: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | LSTM | 809 K \n",
      "-------------------------------\n",
      "809 K     Trainable params\n",
      "0         Non-trainable params\n",
      "809 K     Total params\n",
      "3.238     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 59.65it/s]Validation loss: 2.1165753453510656e+17, Mse: 2.1165751735523738e+17\n",
      "Epoch 0: 100%|██████████| 442/442 [00:07<00:00, 55.80it/s, v_num=11]       Validation loss: 1.4052377937261363e+17, Mse: 1.4052377937261363e+17\n",
      "Epoch 0: 100%|██████████| 442/442 [00:08<00:00, 52.14it/s, v_num=11, val_loss=1.41e+17]Training loss: 324824987598848.0\n",
      "Epoch 1: 100%|██████████| 442/442 [00:08<00:00, 54.73it/s, v_num=11, val_loss=1.41e+17]Validation loss: 1.4052377937261363e+17, Mse: 1.4052377937261363e+17\n",
      "Epoch 1: 100%|██████████| 442/442 [00:08<00:00, 51.49it/s, v_num=11, val_loss=1.41e+17]Training loss: 8.910720545377485e+16\n",
      "Epoch 2: 100%|██████████| 442/442 [00:08<00:00, 54.52it/s, v_num=11, val_loss=1.41e+17]Validation loss: 1.4052377937261363e+17, Mse: 1.4052377937261363e+17\n",
      "Epoch 2: 100%|██████████| 442/442 [00:08<00:00, 51.36it/s, v_num=11, val_loss=1.41e+17]Training loss: 2.4041618887252378e+17\n",
      "Epoch 3:  73%|███████▎  | 324/442 [00:06<00:02, 52.94it/s, v_num=11, val_loss=1.41e+17]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ggil/anaconda3/envs/hackathon/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cfg[\"data_loader\"] = train_loader\n",
    "cfg[\"val_loader\"] = val_loader\n",
    "trainer = pl.Trainer(accelerator=\"cpu\", max_epochs=cfg[\"epochs\"], callbacks=[PrintCallback()])#, EarlyStopping(monitor=\"val_loss\", mode=\"min\")])\n",
    "trainer.fit(model, cfg[\"data_loader\"], cfg[\"val_data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
